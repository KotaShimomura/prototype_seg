{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "controlled-voltage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cython in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (0.28.4)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (1.19.2)\n",
      "Requirement already satisfied: cffi in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.14.3)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (4.4.0.46)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.6.0)\n",
      "Collecting easydict\n",
      "  Downloading easydict-1.10.tar.gz (6.4 kB)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (3.3.4)\n",
      "Requirement already satisfied: Pillow>=6.2.2 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 8)) (9.2.0)\n",
      "Collecting torchcontrib\n",
      "  Downloading torchcontrib-0.0.2.tar.gz (11 kB)\n",
      "Requirement already satisfied: yacs in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (0.1.8)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (5.4.1)\n",
      "Collecting visdom\n",
      "  Downloading visdom-0.2.3.tar.gz (1.4 MB)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Collecting html5lib\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Collecting ninja\n",
      "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
      "Requirement already satisfied: torch>=1.7.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (1.8.0a0+52ea372)\n",
      "Requirement already satisfied: torchvision>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 17)) (0.9.0a0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.8/site-packages (from cffi->-r requirements.txt (line 3)) (2.20)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 7)) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 7)) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 7)) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.8/site-packages (from matplotlib->-r requirements.txt (line 7)) (2.4.7)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from visdom->-r requirements.txt (line 12)) (2.24.0)\n",
      "Requirement already satisfied: tornado in /opt/conda/lib/python3.8/site-packages (from visdom->-r requirements.txt (line 12)) (6.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from visdom->-r requirements.txt (line 12)) (1.15.0)\n",
      "Collecting jsonpatch\n",
      "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
      "Collecting websocket-client\n",
      "  Downloading websocket_client-1.4.2-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from visdom->-r requirements.txt (line 12)) (2.8.5)\n",
      "Requirement already satisfied: pillow-simd in /opt/conda/lib/python3.8/site-packages (from visdom->-r requirements.txt (line 12)) (7.0.0.post3)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.8/site-packages (from bs4->-r requirements.txt (line 13)) (4.9.3)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.8/site-packages (from html5lib->-r requirements.txt (line 14)) (0.5.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.7.0->-r requirements.txt (line 16)) (3.7.4.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->visdom->-r requirements.txt (line 12)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->visdom->-r requirements.txt (line 12)) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->visdom->-r requirements.txt (line 12)) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->visdom->-r requirements.txt (line 12)) (1.25.11)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-2.3-py2.py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in /opt/conda/lib/python3.8/site-packages (from beautifulsoup4->bs4->-r requirements.txt (line 13)) (2.1)\n",
      "Building wheels for collected packages: easydict, torchcontrib, visdom, bs4\n",
      "  Building wheel for easydict (setup.py): started\n",
      "  Building wheel for easydict (setup.py): finished with status 'done'\n",
      "  Created wheel for easydict: filename=easydict-1.10-py3-none-any.whl size=6497 sha256=d9a5f48ddbf6bf3171da65503cc5aa564ac1b85b39597720687c074aa9e22978\n",
      "  Stored in directory: /root/.cache/pip/wheels/fe/4e/02/c9c3154e4845bfdbf1fdf344f5a89f16dcbb4f627a908c9974\n",
      "  Building wheel for torchcontrib (setup.py): started\n",
      "  Building wheel for torchcontrib (setup.py): finished with status 'done'\n",
      "  Created wheel for torchcontrib: filename=torchcontrib-0.0.2-py3-none-any.whl size=7532 sha256=f3984d7c9fd9a89f1bd39b22ed71956d9827a6c198b95d600c853e0a78e2006c\n",
      "  Stored in directory: /root/.cache/pip/wheels/3b/75/17/b11b16ad90276ff6e4e03ec375d55291a186c7fe9dbf87fba3\n",
      "  Building wheel for visdom (setup.py): started\n",
      "  Building wheel for visdom (setup.py): finished with status 'done'\n",
      "  Created wheel for visdom: filename=visdom-0.2.3-py3-none-any.whl size=1417080 sha256=bf95406511da5586ec2b8ece1523b0f3f914c80b6374c4d36219e0fc4ba86b8d\n",
      "  Stored in directory: /root/.cache/pip/wheels/4d/73/32/0bbe55d2dccb9d80d3f020f474c15a5a1eef232817dcebe776\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1273 sha256=615c8a0744ab183bf45a15eba8b2add3e0566f95d03f2d1ba52c80985ce4179b\n",
      "  Stored in directory: /root/.cache/pip/wheels/75/78/21/68b124549c9bdc94f822c02fb9aa3578a669843f9767776bca\n",
      "Successfully built easydict torchcontrib visdom bs4\n",
      "Installing collected packages: easydict, torchcontrib, jsonpointer, jsonpatch, websocket-client, visdom, bs4, html5lib, ninja\n",
      "Successfully installed bs4-0.0.1 easydict-1.10 html5lib-1.1 jsonpatch-1.32 jsonpointer-2.3 ninja-1.11.1 torchcontrib-0.0.2 visdom-0.2.3 websocket-client-1.4.2\n",
      "Collecting pillow==6.2.1\n",
      "  Downloading Pillow-6.2.1-cp38-cp38-manylinux1_x86_64.whl (2.1 MB)\n",
      "Installing collected packages: pillow\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.2.0\n",
      "    Uninstalling Pillow-9.2.0:\n",
      "      Successfully uninstalled Pillow-9.2.0\n",
      "Successfully installed pillow-6.2.1\n",
      "Collecting einops\n",
      "  Downloading einops-0.5.0-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: einops\n",
      "Successfully installed einops-0.5.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "./env.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "collected-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "private-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    model_method = 'fcn'\n",
    "    phase = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "collectible-scroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.models.nets.hrnet import HRNet_W48_Proto\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "DATA_ROOT = '/tmp/working/workspace/ProtoSeg_local/data'\n",
    "SCRATCH_ROOT = '/tmp/working/workspace/ProtoSeg_local/output'\n",
    "ASSET_ROOT = '/tmp/working/workspace/ProtoSeg_local/checkpoints/cityscapes'\n",
    "\n",
    "BACKBONE=\"hrnet48\"\n",
    "DATA_DIR = DATA_ROOT + '/Cityscapes'\n",
    "SAVE_DIR = SCRATCH_ROOT + '/Cityscapes/seg_results/'\n",
    "# SAVE_DIR = '/tmp/working/workspace/PP_seg/output/Cityscapes/seg_results/'\n",
    "\n",
    "CHECKPOINTS_ROOT = SCRATCH_ROOT + \"/Cityscapes\"\n",
    "\n",
    "MODEL_NAME=\"hrnet_w48_proto\"\n",
    "LOSS_TYPE=\"pixel_prototype_ce_loss\"\n",
    "CHECKPOINTS_NAME = \"hrnet_w48_proto_lr1x_hrnet_proto_80k\"\n",
    "\n",
    "PRETRAINED_MODEL = ASSET_ROOT + \"/hrnetv2_w48_imagenet_pretrained.pth\"\n",
    "MAX_ITERS=80000\n",
    "BATCH_SIZE=8\n",
    "BASE_LR=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "included-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): \n",
    "    def str2bool(v):\n",
    "        \"\"\" Usage:\n",
    "        parser.add_argument('--pretrained', type=str2bool, nargs='?', const=True,\n",
    "                            dest='pretrained', help='Whether to use pretrained models.')\n",
    "        \"\"\"\n",
    "        if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
    "            return True\n",
    "        elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
    "            return False\n",
    "        else:\n",
    "            raise argparse.ArgumentTypeError('Unsupported value encountered.')\n",
    "            \n",
    "    # train & test config\n",
    "    parser.add_argument('--phase', default='train', type=str, dest='phase')\n",
    "    parser.add_argument('--model_name', default=\"hrnet_w48_proto\", type=str, dest='model_name')\n",
    "    parser.add_argument('--configs', default='configs/cityscapes/H_48_D_4_proto.json', type=str,dest='configs', help='The file of the hyper parameters.')\n",
    "    parser.add_argument('--gpu', default=[0], nargs='+', type=int, dest='gpu', help='The gpu list used.')\n",
    "    parser.add_argument('--data_dir', default=None, type=str, nargs='+', dest='data:data_dir', help='The Directory of the data.')\n",
    "    parser.add_argument('--backbone', default=None, type=str, dest='network:backbone', help='The base network of model.')\n",
    "    parser.add_argument('--checkpoints_name', default='hrnet_w48_proto_lr1x_hrnet_proto_80k', type=str, dest='checkpoints:checkpoints_name', help='The name of checkpoint model.')\n",
    "    parser.add_argument('--loss_type', default='pixel_prototype_ce_loss', type=str, dest='loss:loss_type', help='The loss type of the network.')\n",
    "    parser.add_argument('--drop_last', type=str2bool, nargs='?', default=False, dest='data:drop_last', help='Fix bug for syncbn.')\n",
    "    \n",
    "    # train config\n",
    "    parser.add_argument('--gathered', type=str2bool, nargs='?', default=True, dest='network:gathered', help='Whether to gather the output of model.')\n",
    "    parser.add_argument('--loss_balance', type=str2bool, nargs='?', default=False, dest='network:loss_balance', help='Whether to balance GPU usage.')\n",
    "    parser.add_argument('--log_to_file', type=str2bool, nargs='?', default=True, dest='logging:log_to_file', help='Whether to write logging into files.')\n",
    "    parser.add_argument('--max_iters', default=None, type=int, dest='solver:max_iters', help='The max iters of training.')\n",
    "    parser.add_argument('--checkpoints_root', default=None, type=str, dest='checkpoints:checkpoints_root', help='The root dir of model save path.')\n",
    "    parser.add_argument('--pretrained', type=str, default=None, dest='network:pretrained', help='The path to pretrained model.')\n",
    "    parser.add_argument('--train_batch_size', default=None, type=int, dest='train:batch_size', help='The batch size of training.')\n",
    "    parser.add_argument('--distributed', action='store_true', dest='distributed', help='Use multi-processing training.')\n",
    "    parser.add_argument('--base_lr', default=None, type=float, dest='lr:base_lr', help='The learning rate.')\n",
    "    parser.add_argument('--nbb_mult', default=1.0, type=float, dest='lr:nbb_mult', help='The not backbone mult ratio of learning rate.')\n",
    "\n",
    "    # test config\n",
    "    parser.add_argument('--network', default='hrnet_w48_proto', type=str, dest='network')\n",
    "    parser.add_argument('REMAIN', nargs='*')\n",
    "    parser.add_argument('--test_img', default=None, type=str, dest='test:test_img', help='The test path of image.')\n",
    "    parser.add_argument('--test_dir', default=None, type=str, dest='test:test_dir', help='The test directory of images.')\n",
    "    parser.add_argument('--out_dir', default='none', type=str, dest='test:out_dir', help='The test out directory of images.')\n",
    "    parser.add_argument('--save_prob', type=str2bool, nargs='?', default=False, dest='test:save_prob', help='Save the logits map during testing.')\n",
    "    parser.add_argument('--resume', default=None, type=str, dest='network:resume', help='The path of checkpoints.')\n",
    "\n",
    "    print('pass parser set')\n",
    "    \n",
    "    # python\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    # jupyter\n",
    "    # args_parser = parser.parse_args(args=['--phase',\n",
    "    #                                       '--model_method',\n",
    "    #                                       '--network',\n",
    "    #                                       '--model_name'])\n",
    "\n",
    "    args_parser = parser.parse_args(args=['--drop_last','y',\n",
    "                                         '--gathered','n',\n",
    "                                         '--loss_balance','y',\n",
    "                                         '--log_to_file','n',\n",
    "                                         '--data_dir',DATA_DIR,\n",
    "                                         '--backbone', BACKBONE,\n",
    "                                         '--model_name',MODEL_NAME,\n",
    "                                         '--loss_type ',LOSS_TYPE,\n",
    "                                         '--max_iters', '80000',\n",
    "                                         '--checkpoints_root', CHECKPOINTS_ROOT,\n",
    "                                         '--pretrained',PRETRAINED_MODEL,\n",
    "                                         '--train_batch_size', '8',\n",
    "                                         '--distributed',\n",
    "                                         '--base_lr', '0.01',])\n",
    "    \n",
    "    return args_parser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-allen",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "loved-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "class Configer(object):\n",
    "\n",
    "    def __init__(self, args_parser=None, configs=None, config_dict=None):\n",
    "        if config_dict is not None:\n",
    "            self.params_root = config_dict\n",
    "\n",
    "        elif configs is not None:\n",
    "            if not os.path.exists(configs):\n",
    "                exit(0)\n",
    "\n",
    "            json_stream = open(configs, 'r')\n",
    "            self.params_root = json.load(json_stream)\n",
    "            json_stream.close()\n",
    "\n",
    "        elif args_parser is not None:\n",
    "            self.args_dict = args_parser.__dict__\n",
    "            self.params_root = None\n",
    "\n",
    "            if not os.path.exists(args_parser.configs):\n",
    "                print('Json Path:{} not exists!'.format(args_parser.configs))\n",
    "                exit(1)\n",
    "\n",
    "            json_stream = open(args_parser.configs, 'r')\n",
    "            self.params_root = json.load(json_stream)\n",
    "            json_stream.close()\n",
    "\n",
    "            for key, value in self.args_dict.items():\n",
    "                if not self.exists(*key.split(':')):\n",
    "                    self.add(key.split(':'), value)\n",
    "                elif value is not None:\n",
    "                    self.update(key.split(':'), value)\n",
    "\n",
    "            self._handle_remaining_args(args_parser.REMAIN)\n",
    "\n",
    "        self.conditions = _ConditionHelper(self)\n",
    "\n",
    "\n",
    "    def _handle_remaining_args(self, remain):\n",
    "\n",
    "        def _parse_value(x: str):\n",
    "            \"\"\"\n",
    "            We first try to parse `x` as python literal object.\n",
    "            If failed, we regard x as string.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                return literal_eval(x)\n",
    "            except ValueError:\n",
    "                return x\n",
    "\n",
    "        def _set_value(key, value):\n",
    "            \"\"\"\n",
    "            We directly operate on `params_root`.\n",
    "            \"\"\"\n",
    "            remained_parts = key.split('.')\n",
    "            consumed_parts = []\n",
    "\n",
    "            parent_dict = self.params_root\n",
    "            while len(remained_parts) > 1:\n",
    "                cur_key = remained_parts.pop(0)\n",
    "                consumed_parts.append(cur_key)\n",
    "\n",
    "                if cur_key not in parent_dict:\n",
    "                    parent_dict[cur_key] = dict()\n",
    "                elif not isinstance(parent_dict[cur_key], dict):\n",
    "                    sys.exit(1)\n",
    "                \n",
    "                parent_dict = parent_dict[cur_key]\n",
    "\n",
    "            cur_key = remained_parts.pop(0)\n",
    "            consumed_parts.append(cur_key)\n",
    "\n",
    "            if cur_key.endswith('+'):\n",
    "                cur_key = cur_key[:-1]\n",
    "                target = parent_dict.get(cur_key)\n",
    "\n",
    "                if not isinstance(target, list):\n",
    "                    sys.exit(1)\n",
    "\n",
    "                target.append(value)\n",
    "                return\n",
    "\n",
    "            existing_value = parent_dict.get(cur_key)\n",
    "#             if existing_value is not None:\n",
    "#             else:\n",
    "            parent_dict[cur_key] = value\n",
    "\n",
    "        assert len(remain) % 2 == 0, remain\n",
    "        args = {}\n",
    "        for i in range(len(remain) // 2):\n",
    "            key, value = remain[2 * i: 2 * i + 2]\n",
    "            _set_value(key, _parse_value(value))\n",
    "\n",
    "    def clone(self):\n",
    "        from copy import deepcopy\n",
    "        return Configer(config_dict=deepcopy(self.params_root))\n",
    "\n",
    "    def _get_caller(self):\n",
    "        filename = os.path.basename(sys._getframe().f_back.f_back.f_code.co_filename)\n",
    "        lineno = sys._getframe().f_back.f_back.f_lineno\n",
    "        prefix = '{}, {}'.format(filename, lineno)\n",
    "        return prefix\n",
    "\n",
    "    def get(self, *key):\n",
    "        if len(key) == 0:\n",
    "            return self.params_root\n",
    "\n",
    "        elif len(key) == 1:\n",
    "            if key[0] in self.params_root:\n",
    "                return self.params_root[key[0]]\n",
    "            else:\n",
    "                exit(1)\n",
    "\n",
    "        elif len(key) == 2:\n",
    "            if key[0] in self.params_root and key[1] in self.params_root[key[0]]:\n",
    "                return self.params_root[key[0]][key[1]]\n",
    "            else:\n",
    "                exit(1)\n",
    "\n",
    "        else:\n",
    "            exit(1)\n",
    "\n",
    "    def exists(self, *key):\n",
    "        if len(key) == 1 and key[0] in self.params_root:\n",
    "            return True\n",
    "\n",
    "        if len(key) == 2 and (key[0] in self.params_root and key[1] in self.params_root[key[0]]):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def add(self, key_tuple, value):\n",
    "        if self.exists(*key_tuple):\n",
    "            exit(1)\n",
    "\n",
    "        if len(key_tuple) == 1:\n",
    "            self.params_root[key_tuple[0]] = value\n",
    "\n",
    "        elif len(key_tuple) == 2:\n",
    "            if key_tuple[0] not in self.params_root:\n",
    "                self.params_root[key_tuple[0]] = dict()\n",
    "\n",
    "            self.params_root[key_tuple[0]][key_tuple[1]] = value\n",
    "\n",
    "        else:\n",
    "            exit(1)\n",
    "\n",
    "    def update(self, key_tuple, value):\n",
    "        if not self.exists(*key_tuple):\n",
    "            exit(1)\n",
    "\n",
    "        if len(key_tuple) == 1 and not isinstance(self.params_root[key_tuple[0]], dict):\n",
    "            self.params_root[key_tuple[0]] = value\n",
    "\n",
    "        elif len(key_tuple) == 2:\n",
    "            self.params_root[key_tuple[0]][key_tuple[1]] = value\n",
    "\n",
    "        else:\n",
    "            exit(1)\n",
    "\n",
    "    def resume(self, config_dict):\n",
    "        self.params_root = config_dict\n",
    "\n",
    "    def plus_one(self, *key):\n",
    "        if not self.exists(*key):\n",
    "            exit(1)\n",
    "\n",
    "        if len(key) == 1 and not isinstance(self.params_root[key[0]], dict):\n",
    "            self.params_root[key[0]] += 1\n",
    "\n",
    "        elif len(key) == 2:\n",
    "            self.params_root[key[0]][key[1]] += 1\n",
    "\n",
    "        else:\n",
    "            exit(1)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.params_root\n",
    "\n",
    "\n",
    "class _ConditionHelper:\n",
    "    \"\"\"Handy helper\"\"\"\n",
    "\n",
    "    def __init__(self, configer):\n",
    "        self.configer = configer\n",
    "\n",
    "    @property\n",
    "    def use_multi_dataset(self):\n",
    "        root_dirs = self.configer.get('data', 'data_dir')\n",
    "        return isinstance(root_dirs, (tuple, list)) and len(root_dirs) > 1\n",
    "\n",
    "    @property\n",
    "    def pred_sw_offset(self):\n",
    "        return self.configer.exists('data', 'pred_sw_offset')\n",
    "\n",
    "    @property\n",
    "    def pred_dt_offset(self):\n",
    "        return self.configer.exists('data', 'pred_dt_offset')\n",
    "\n",
    "    @property\n",
    "    def use_sw_offset(self):\n",
    "        return self.configer.exists('data', 'use_sw_offset')\n",
    "\n",
    "    @property\n",
    "    def use_dt_offset(self):\n",
    "        return self.configer.exists('data', 'use_dt_offset')\n",
    "\n",
    "    @property\n",
    "    def use_ground_truth(self):\n",
    "        return self.config_equals(('use_ground_truth',), True)\n",
    "\n",
    "    @property\n",
    "    def pred_ml_dt_offset(self):\n",
    "        return self.configer.exists('data', 'pred_ml_dt_offset')\n",
    "\n",
    "    def loss_contains(self, name):\n",
    "        return name in self.configer.get('loss', 'loss_type')\n",
    "\n",
    "    def model_contains(self, name):\n",
    "        return name in self.configer.get('network', 'model_name')\n",
    "\n",
    "    def config_equals(self, key, value):\n",
    "        if not self.configer.exists(*key):\n",
    "            return False\n",
    "\n",
    "        return self.configer.get(*key) == value\n",
    "\n",
    "    def config_exists(self, key):\n",
    "        return self.configer.exists(*key)\n",
    "\n",
    "    def environ_exists(self, key):\n",
    "        return os.environ.get(key) is not None\n",
    "\n",
    "    @property\n",
    "    def diverse_size(self):\n",
    "        return self.configer.get('val', 'data_transformer')['size_mode'] == 'diverse_size'\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\" Computes ans stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0.\n",
    "        self.avg = 0.\n",
    "        self.sum = 0.\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-country",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "lasting-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "import lib.datasets.tools.transforms as trans\n",
    "import lib.datasets.tools.cv2_aug_transforms as cv2_aug_trans\n",
    "import lib.datasets.tools.pil_aug_transforms as pil_aug_trans\n",
    "from lib.datasets.loader.default_loader import DefaultLoader, CSDataTestLoader\n",
    "from lib.datasets.tools.collate import collate\n",
    "from lib.utils.tools.logger import Logger as Log\n",
    "\n",
    "from lib.utils.distributed import get_world_size, get_rank, is_distributed\n",
    "\n",
    "class DataLoader(object):\n",
    "\n",
    "    def __init__(self, configer):\n",
    "        self.configer = configer\n",
    "\n",
    "        from lib.datasets.tools import cv2_aug_transforms\n",
    "        self.aug_train_transform = cv2_aug_transforms.CV2AugCompose(self.configer, split='train')\n",
    "        self.aug_val_transform = cv2_aug_transforms.CV2AugCompose(self.configer, split='val')\n",
    "\n",
    "        self.img_transform = trans.Compose([\n",
    "            trans.ToTensor(),\n",
    "            trans.Normalize(div_value=self.configer.get('normalize', 'div_value'),\n",
    "                            mean=self.configer.get('normalize', 'mean'),\n",
    "                            std=self.configer.get('normalize', 'std')), ])\n",
    "\n",
    "        self.label_transform = trans.Compose([\n",
    "            trans.ToLabel(),\n",
    "            trans.ReLabel(255, -1), ])\n",
    " \n",
    "    def get_dataloader_sampler(self, klass, split, dataset):\n",
    "\n",
    "        from lib.datasets.loader.multi_dataset_loader import MultiDatasetLoader, MultiDatasetTrainingSampler\n",
    "\n",
    "        root_dir = self.configer.get('data', 'data_dir')\n",
    "        if isinstance(root_dir, list) and len(root_dir) == 1:\n",
    "            root_dir = root_dir[0]\n",
    "\n",
    "        kwargs = dict(\n",
    "            dataset=dataset,\n",
    "            aug_transform=(self.aug_train_transform if split == 'train' else self.aug_val_transform),\n",
    "            img_transform=self.img_transform,\n",
    "            label_transform=self.label_transform,\n",
    "            configer=self.configer\n",
    "        )\n",
    "\n",
    "        if isinstance(root_dir, str):\n",
    "            loader = klass(root_dir, **kwargs)\n",
    "            multi_dataset = False\n",
    "        elif isinstance(root_dir, list):\n",
    "            loader = MultiDatasetLoader(root_dir, klass, **kwargs)\n",
    "            multi_dataset = True\n",
    "            Log.info('use multi-dataset for {}...'.format(dataset))\n",
    "        else:\n",
    "            raise RuntimeError('Unknown root dir {}'.format(root_dir))\n",
    "\n",
    "        if split == 'train':\n",
    "            if is_distributed() and multi_dataset:\n",
    "                raise RuntimeError('Currently multi dataset doesn\\'t support distributed.')\n",
    "\n",
    "            if is_distributed():\n",
    "                sampler = torch.utils.data.distributed.DistributedSampler(loader)\n",
    "            elif multi_dataset:\n",
    "                sampler = MultiDatasetTrainingSampler(loader)\n",
    "            else:\n",
    "                sampler = None\n",
    "\n",
    "        elif split == 'val':\n",
    "\n",
    "            if is_distributed():\n",
    "                sampler = torch.utils.data.distributed.DistributedSampler(loader)\n",
    "            else:\n",
    "                sampler = None\n",
    "\n",
    "        return loader, sampler\n",
    "\n",
    "    def get_trainloader(self):\n",
    "        if self.configer.exists('data', 'use_edge') and self.configer.get('data', 'use_edge') == 'ce2p':\n",
    "            \"\"\"\n",
    "            ce2p manner:\n",
    "            load both the ground-truth label and edge.\n",
    "            \"\"\"\n",
    "            Log.info('use edge (follow ce2p) for train...')\n",
    "            klass = LipLoader\n",
    "\n",
    "        elif self.configer.exists('data', 'use_dt_offset') or self.configer.exists('data', 'pred_dt_offset'):\n",
    "            \"\"\"\n",
    "            dt-offset manner:\n",
    "            load both the ground-truth label and offset (based on distance transform).\n",
    "            \"\"\"\n",
    "            Log.info('use distance transform offset loader for train...')\n",
    "            klass = DTOffsetLoader\n",
    "\n",
    "        elif self.configer.exists('train', 'loader') and \\\n",
    "                (self.configer.get('train', 'loader') == 'ade20k'\n",
    "                 or self.configer.get('train', 'loader') == 'pascal_context'\n",
    "                 or self.configer.get('train', 'loader') == 'pascal_voc'\n",
    "                 or self.configer.get('train', 'loader') == 'coco_stuff'\n",
    "                 or self.configer.get('train', 'loader') == 'camvid'):\n",
    "            \"\"\"\n",
    "            ADE20KLoader manner:\n",
    "            support input images of different shapes.\n",
    "            \"\"\"\n",
    "            Log.info('use ADE20KLoader (diverse input shape) for train...')\n",
    "            klass = ADE20KLoader\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Default manner:\n",
    "            + support input images of the same shapes.\n",
    "            + support distributed training (the performance is more un-stable than non-distributed manner)\n",
    "            \"\"\"\n",
    "            Log.info('use the DefaultLoader for train...')\n",
    "            klass = DefaultLoader\n",
    "        loader, sampler = self.get_dataloader_sampler(klass, 'train', 'train')\n",
    "        trainloader = data.DataLoader(\n",
    "            loader,\n",
    "            batch_size=self.configer.get('train', 'batch_size') // get_world_size(), pin_memory=True,\n",
    "            num_workers=self.configer.get('data', 'workers') // get_world_size(),\n",
    "            sampler=sampler,\n",
    "            shuffle=(sampler is None),\n",
    "            drop_last=self.configer.get('data', 'drop_last'),\n",
    "            collate_fn=lambda *args: collate(\n",
    "                *args, trans_dict=self.configer.get('train', 'data_transformer')\n",
    "            )\n",
    "        )\n",
    "        return trainloader\n",
    "    \n",
    "    def get_valloader(self, dataset=None):\n",
    "        dataset = 'val' if dataset is None else dataset\n",
    "\n",
    "        if self.configer.exists('data', 'use_dt_offset') or self.configer.exists('data', 'pred_dt_offset'):\n",
    "            \"\"\"\n",
    "            dt-offset manner:\n",
    "            load both the ground-truth label and offset (based on distance transform).\n",
    "            \"\"\"\n",
    "            Log.info('use distance transform based offset loader for val ...')\n",
    "            klass = DTOffsetLoader\n",
    "\n",
    "        elif self.configer.get('method') == 'fcn_segmentor':\n",
    "            \"\"\"\n",
    "            default manner:\n",
    "            load the ground-truth label.\n",
    "            \"\"\"\n",
    "            Log.info('use DefaultLoader for val ...')\n",
    "            klass = DefaultLoader\n",
    "        else:\n",
    "            Log.error('Method: {} loader is invalid.'.format(self.configer.get('method')))\n",
    "            return None\n",
    "\n",
    "        loader, sampler = self.get_dataloader_sampler(klass, 'val', dataset)\n",
    "        valloader = data.DataLoader(\n",
    "            loader,\n",
    "            sampler=sampler,\n",
    "            batch_size=self.configer.get('val', 'batch_size') // get_world_size(), pin_memory=False,\n",
    "            num_workers=self.configer.get('data', 'workers'), shuffle=False,\n",
    "            collate_fn=lambda *args: collate(\n",
    "                *args, trans_dict=self.configer.get('val', 'data_transformer')\n",
    "            )\n",
    "        )\n",
    "        return valloader\n",
    "\n",
    "    def get_testloader(self, dataset=None):\n",
    "        dataset = 'test' if dataset is None else dataset\n",
    "        if self.configer.exists('data', 'use_sw_offset') or self.configer.exists('data', 'pred_sw_offset'):\n",
    "            Log.info('use sliding window based offset loader for test ...')\n",
    "            test_loader = data.DataLoader(\n",
    "                SWOffsetTestLoader(root_dir=self.configer.get('data', 'data_dir'), dataset=dataset,\n",
    "                                   img_transform=self.img_transform,\n",
    "                                   configer=self.configer),\n",
    "                batch_size=self.configer.get('test', 'batch_size'), pin_memory=False,\n",
    "                num_workers=self.configer.get('data', 'workers'), shuffle=False,\n",
    "                collate_fn=lambda *args: collate(\n",
    "                    *args, trans_dict=self.configer.get('test', 'data_transformer')\n",
    "                )\n",
    "            )\n",
    "            return test_loader\n",
    "\n",
    "        elif self.configer.get('method') == 'fcn_segmentor':\n",
    "            Log.info('use CSDataTestLoader for test ...')\n",
    "\n",
    "            root_dir = self.configer.get('data', 'data_dir')\n",
    "            if isinstance(root_dir, list) and len(root_dir) == 1:\n",
    "                root_dir = root_dir[0]\n",
    "            test_loader = data.DataLoader(\n",
    "                CSDataTestLoader(root_dir=root_dir, dataset=dataset,\n",
    "                                 img_transform=self.img_transform,\n",
    "                                 configer=self.configer),\n",
    "                batch_size=self.configer.get('test', 'batch_size'), pin_memory=True,\n",
    "                num_workers=self.configer.get('data', 'workers'), shuffle=False,\n",
    "                collate_fn=lambda *args: collate(\n",
    "                    *args, trans_dict=self.configer.get('test', 'data_transformer')\n",
    "                )\n",
    "            )\n",
    "            return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "optical-modeling",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.models.nets.hrnet import HRNet_W48_Proto\n",
    "\n",
    "SEG_MODEL_DICT = {\n",
    "    'hrnet_w48_proto': HRNet_W48_Proto\n",
    "}\n",
    "\n",
    "class Segmentation_Model(object):\n",
    "    def __init__(self, configer):\n",
    "        self.configer = configer\n",
    "\n",
    "    def semantic_segmentor(self):\n",
    "        # ここよくわからん\n",
    "        #model_name = self.configer.get('network', 'model_name')\n",
    "        model_name = 'hrnet_w48_proto'\n",
    "\n",
    "        if model_name not in SEG_MODEL_DICT:\n",
    "            print(\"your select model not found\")\n",
    "        print(model_name)\n",
    "        model = SEG_MODEL_DICT[model_name](self.configer)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-perth",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brave-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.loss.loss_proto import PixelPrototypeCELoss\n",
    "\n",
    "SEG_LOSS_DICT = {\n",
    "    'pixel_prototype_ce_loss': PixelPrototypeCELoss\n",
    "}\n",
    "\n",
    "class LossManager(object):\n",
    "    def __init__(self, configer):\n",
    "        self.configer = configer\n",
    "\n",
    "    def _parallel(self, loss):\n",
    "        if is_distributed():\n",
    "            print('use distributed loss')\n",
    "            return loss\n",
    "\n",
    "    def get_seg_loss(self, loss_type=None):\n",
    "        key = self.configer.get('loss', 'loss_type') if loss_type is None else loss_type\n",
    "        if key not in SEG_LOSS_DICT:\n",
    "            Log.error('Loss: {} not valid!'.format(key))\n",
    "            exit(1)\n",
    "        Log.info('use loss: {}.'.format(key))\n",
    "        loss = SEG_LOSS_DICT[key](self.configer)\n",
    "        return self._parallel(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-brooklyn",
   "metadata": {},
   "source": [
    "# optim & scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "comparable-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torchcontrib\n",
    "from torch.optim import SGD, Adam, AdamW, lr_scheduler\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "from lib.utils.tools.logger import Logger as Log\n",
    "\n",
    "\n",
    "class WarmupCosineSchedule(LambdaLR):\n",
    "    \"\"\" Linear warmup and then cosine decay.\n",
    "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
    "        Decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps following a cosine curve.\n",
    "        If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.t_total = t_total\n",
    "        self.cycles = cycles\n",
    "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
    "\n",
    "    def lr_lambda(self, step):\n",
    "        if step < self.warmup_steps:\n",
    "            return float(step) / float(max(1.0, self.warmup_steps))\n",
    "        # progress after warmup\n",
    "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
    "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))\n",
    "\n",
    "\n",
    "class OptimScheduler(object):\n",
    "    def __init__(self, configer):\n",
    "        self.configer = configer\n",
    "\n",
    "    def init_optimizer(self, net_params):\n",
    "        optimizer = None\n",
    "        if self.configer.get('optim', 'optim_method') == 'sgd':\n",
    "            optimizer = SGD(net_params,\n",
    "                            lr=self.configer.get('lr', 'base_lr'),\n",
    "                            momentum=self.configer.get('optim', 'sgd')['momentum'],\n",
    "                            weight_decay=self.configer.get('optim', 'sgd')['weight_decay'],\n",
    "                            nesterov=self.configer.get('optim', 'sgd')['nesterov'])\n",
    "\n",
    "        elif self.configer.get('optim', 'optim_method') == 'adam':\n",
    "            optimizer = Adam(net_params,\n",
    "                             lr=self.configer.get('lr', 'base_lr'),\n",
    "                             betas=self.configer.get('optim', 'adam')['betas'],\n",
    "                             eps=self.configer.get('optim', 'adam')['eps'],\n",
    "                             weight_decay=self.configer.get('optim', 'adam')['weight_decay'])\n",
    "        elif self.configer.get('optim', 'optim_method') == 'adamw':\n",
    "            optimizer = AdamW(net_params,\n",
    "                              lr=self.configer.get('lr', 'base_lr'),\n",
    "                              betas=self.configer.get('optim', 'adamw')['betas'],\n",
    "                              eps=self.configer.get('optim', 'adamw')['eps'],\n",
    "                              weight_decay=self.configer.get('optim', 'adamw')['weight_decay'])\n",
    "\n",
    "        else:\n",
    "            Log.error('Optimizer {} is not valid.'.format(self.configer.get('optim', 'optim_method')))\n",
    "            exit(1)\n",
    "\n",
    "        policy = self.configer.get('lr', 'lr_policy')\n",
    "\n",
    "        scheduler = None\n",
    "        if policy == 'step':\n",
    "            scheduler = lr_scheduler.StepLR(optimizer,\n",
    "                                            self.configer.get('lr', 'step')['step_size'],\n",
    "                                            gamma=self.configer.get('lr', 'step')['gamma'])\n",
    "\n",
    "        elif policy == 'multistep':\n",
    "            scheduler = lr_scheduler.MultiStepLR(optimizer,\n",
    "                                                 self.configer.get('lr', 'multistep')['stepvalue'],\n",
    "                                                 gamma=self.configer.get('lr', 'multistep')['gamma'])\n",
    "\n",
    "        elif policy == 'lambda_poly':\n",
    "            if os.environ.get('lambda_poly_power'):\n",
    "                _lambda_poly_power = float(os.environ.get('lambda_poly_power'))\n",
    "                Log.info('Use lambda_poly policy with power {}'.format(_lambda_poly_power))\n",
    "                lambda_poly = lambda iters: pow((1.0 - iters / self.configer.get('solver', 'max_iters')),\n",
    "                                                _lambda_poly_power)\n",
    "            elif self.configer.exists('lr', 'lambda_poly'):\n",
    "                Log.info('Use lambda_poly policy with power {}'.format(self.configer.get('lr', 'lambda_poly')['power']))\n",
    "                lambda_poly = lambda iters: pow((1.0 - iters / self.configer.get('solver', 'max_iters')),\n",
    "                                                self.configer.get('lr', 'lambda_poly')['power'])\n",
    "            else:\n",
    "                Log.info('Use lambda_poly policy with default power 0.9')\n",
    "                lambda_poly = lambda iters: pow((1.0 - iters / self.configer.get('solver', 'max_iters')), 0.9)\n",
    "            scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_poly)\n",
    "\n",
    "        elif policy == 'lambda_cosine':\n",
    "            lambda_cosine = lambda iters: (math.cos(math.pi * iters / self.configer.get('solver', 'max_iters'))\n",
    "                                           + 1.0) / 2\n",
    "            scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_cosine)\n",
    "\n",
    "        elif policy == 'plateau':\n",
    "            scheduler = lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                       mode=self.configer.get('lr', 'plateau')['mode'],\n",
    "                                                       factor=self.configer.get('lr', 'plateau')['factor'],\n",
    "                                                       patience=self.configer.get('lr', 'plateau')['patience'],\n",
    "                                                       threshold=self.configer.get('lr', 'plateau')['threshold'],\n",
    "                                                       threshold_mode=self.configer.get('lr', 'plateau')['thre_mode'],\n",
    "                                                       cooldown=self.configer.get('lr', 'plateau')['cooldown'],\n",
    "                                                       min_lr=self.configer.get('lr', 'plateau')['min_lr'],\n",
    "                                                       eps=self.configer.get('lr', 'plateau')['eps'])\n",
    "\n",
    "        elif policy == 'swa_lambda_poly':\n",
    "            optimizer = torchcontrib.optim.SWA(optimizer)\n",
    "            normal_max_iters = int(self.configer.get('solver', 'max_iters') * 0.75)\n",
    "            swa_step_max_iters = (self.configer.get('solver',\n",
    "                                                    'max_iters') - normal_max_iters) // 5 + 1  # we use 5 ensembles here\n",
    "\n",
    "            def swa_lambda_poly(iters):\n",
    "                if iters < normal_max_iters:\n",
    "                    return pow(1.0 - iters / normal_max_iters, 0.9)\n",
    "                else:  # set lr to half of initial lr and start swa\n",
    "                    return 0.5 * pow(1.0 - ((iters - normal_max_iters) % swa_step_max_iters) / swa_step_max_iters, 0.9)\n",
    "\n",
    "            scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=swa_lambda_poly)\n",
    "\n",
    "        elif policy == 'swa_lambda_cosine':\n",
    "            optimizer = torchcontrib.optim.SWA(optimizer)\n",
    "            normal_max_iters = int(self.configer.get('solver', 'max_iters') * 0.75)\n",
    "            swa_step_max_iters = (self.configer.get('solver',\n",
    "                                                    'max_iters') - normal_max_iters) // 5 + 1  # we use 5 ensembles here\n",
    "\n",
    "            def swa_lambda_cosine(iters):\n",
    "                if iters < normal_max_iters:\n",
    "                    return (math.cos(math.pi * iters / normal_max_iters) + 1.0) / 2\n",
    "                else:  # set lr to half of initial lr and start swa\n",
    "                    return 0.5 * (math.cos(\n",
    "                        math.pi * ((iters - normal_max_iters) % swa_step_max_iters) / swa_step_max_iters) + 1.0) / 2\n",
    "\n",
    "            scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=swa_lambda_cosine)\n",
    "\n",
    "        elif policy == 'warmup_cosine':\n",
    "            scheduler = WarmupCosineSchedule(optimizer, warmup_steps=1000,\n",
    "                                             t_total=self.configer.get('solver', 'max_iters'))\n",
    "\n",
    "        else:\n",
    "            Log.error('Policy:{} is not valid.'.format(policy))\n",
    "            exit(1)\n",
    "\n",
    "        return optimizer, scheduler\n",
    "\n",
    "    def update_optimizer(self, net, optim_method, lr_policy):\n",
    "        self.configer.update(('optim', 'optim_method'), optim_method)\n",
    "        self.configer.update(('lr', 'lr_policy'), lr_policy)\n",
    "        optimizer, scheduler = self.init_optimizer(net)\n",
    "        return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-control",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "phantom-plasma",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 15:41:21,959 INFO    [offset_helper.py, 51] engery/max-distance: 5 engery/min-distance: 0\n",
      "2022-11-06 15:41:21,959 INFO    [offset_helper.py, 58] direction/num_classes: 8 scale: 1\n",
      "2022-11-06 15:41:21,960 INFO    [offset_helper.py, 65] c4 align axis: False\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import pdb\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "# from lib.models.model_manager import ModelManager\n",
    "from lib.utils.tools.logger import Logger as Log\n",
    "from lib.vis.seg_visualizer import SegVisualizer\n",
    "from segmentor.tools.module_runner import ModuleRunner\n",
    "from segmentor.tools.data_helper import DataHelper\n",
    "from segmentor.tools.evaluator import get_evaluator\n",
    "from lib.utils.distributed import get_world_size, get_rank, is_distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fifty-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self, configer):\n",
    "        self.configer = configer\n",
    "        self.batch_time = AverageMeter()\n",
    "        self.foward_time = AverageMeter()\n",
    "        self.backward_time = AverageMeter()\n",
    "        self.loss_time = AverageMeter()\n",
    "        self.data_time = AverageMeter()\n",
    "        self.train_losses = AverageMeter()\n",
    "        # self.val_losses = AverageMeter()\n",
    "        self.seg_visualizer = SegVisualizer(configer)\n",
    "        self.loss_manager = LossManager(configer)\n",
    "        self.module_runner = ModuleRunner(configer)\n",
    "        self.model_manager = Segmentation_Model(configer)\n",
    "        self.data_loader = DataLoader(configer)\n",
    "        self.optim_scheduler = OptimScheduler(configer)\n",
    "        self.data_helper = DataHelper(configer, self)\n",
    "        self.evaluator = get_evaluator(configer, self)\n",
    "\n",
    "        self.seg_net = None\n",
    "        self.train_loader = None\n",
    "        self.val_loader = None\n",
    "        self.optimizer = None\n",
    "        self.scheduler = None\n",
    "        self.running_score = None\n",
    "\n",
    "        self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        self.seg_net = self.model_manager.semantic_segmentor()\n",
    "\n",
    "        try:\n",
    "            flops, params = get_model_complexity_info(self.seg_net, (3, 512, 512))\n",
    "            split_line = '=' * 30\n",
    "            print('{0}\\nInput shape: {1}\\nFlops: {2}\\nParams: {3}\\n{0}'.format(\n",
    "                split_line, (3, 512, 512), flops, params))\n",
    "            print('!!!Please be cautious if you use the results in papers. '\n",
    "                  'You may need to check if all ops are supported and verify that the '\n",
    "                  'flops computation is correct.')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        self.seg_net = self.module_runner.load_net(self.seg_net)\n",
    "\n",
    "        if self.configer.get('optim', 'group_method') == 'decay':\n",
    "            params_group = self.group_weight(self.seg_net)\n",
    "        else:\n",
    "            assert self.configer.get('optim', 'group_method') is None\n",
    "            params_group = self._get_parameters()\n",
    "\n",
    "        self.optimizer, self.scheduler = self.optim_scheduler.init_optimizer(params_group)\n",
    "\n",
    "        self.train_loader = self.data_loader.get_trainloader()\n",
    "        self.val_loader = self.data_loader.get_valloader()\n",
    "        self.pixel_loss = self.loss_manager.get_seg_loss()\n",
    "        if is_distributed():\n",
    "            self.pixel_loss = self.module_runner.to_device(self.pixel_loss)\n",
    "\n",
    "        self.with_proto = True if self.configer.exists(\"protoseg\") else False\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def group_weight(module):\n",
    "        group_decay = []\n",
    "        group_no_decay = []\n",
    "        for m in module.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                group_decay.append(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    group_no_decay.append(m.bias)\n",
    "            elif isinstance(m, nn.modules.conv._ConvNd):\n",
    "                group_decay.append(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    group_no_decay.append(m.bias)\n",
    "            else:\n",
    "                if hasattr(m, 'weight'):\n",
    "                    group_no_decay.append(m.weight)\n",
    "                if hasattr(m, 'bias'):\n",
    "                    group_no_decay.append(m.bias)\n",
    "\n",
    "        assert len(list(module.parameters())) == len(group_decay) + len(group_no_decay)\n",
    "        groups = [dict(params=group_decay), dict(params=group_no_decay, weight_decay=.0)]\n",
    "        return groups\n",
    "    \n",
    "    def _get_parameters(self):\n",
    "        bb_lr = []\n",
    "        nbb_lr = []\n",
    "        fcn_lr = []\n",
    "        params_dict = dict(self.seg_net.named_parameters())\n",
    "        for key, value in params_dict.items():\n",
    "            if 'backbone' in key:\n",
    "                bb_lr.append(value)\n",
    "            elif 'aux_layer' in key or 'upsample_proj' in key:\n",
    "                fcn_lr.append(value)\n",
    "            else:\n",
    "                nbb_lr.append(value)\n",
    "        params = [{'params': bb_lr, 'lr': self.configer.get('lr', 'base_lr')},\n",
    "                  {'params': fcn_lr, 'lr': self.configer.get('lr', 'base_lr') * 10},\n",
    "                  {'params': nbb_lr, 'lr': self.configer.get('lr', 'base_lr') * self.configer.get('lr', 'nbb_mult')}]\n",
    "        return params\n",
    "    \n",
    "    def __train(self):\n",
    "        \"\"\"\n",
    "          Train function of every epoch during train phase.\n",
    "        \"\"\"\n",
    "        self.seg_net.train()\n",
    "        self.pixel_loss.train()\n",
    "        start_time = time.time()\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "        if \"swa\" in self.configer.get('lr', 'lr_policy'):\n",
    "            normal_max_iters = int(self.configer.get('solver', 'max_iters') * 0.75)\n",
    "            swa_step_max_iters = (self.configer.get('solver', 'max_iters') - normal_max_iters) // 5 + 1\n",
    "\n",
    "        if hasattr(self.train_loader.sampler, 'set_epoch'):\n",
    "            self.train_loader.sampler.set_epoch(self.configer.get('epoch'))\n",
    "\n",
    "        for i, data_dict in enumerate(self.train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            if self.configer.get('lr', 'metric') == 'iters':\n",
    "                self.scheduler.step(self.configer.get('iters'))\n",
    "            else:\n",
    "                self.scheduler.step(self.configer.get('epoch'))\n",
    "\n",
    "            if self.configer.get('lr', 'is_warm'):\n",
    "                self.module_runner.warm_lr(\n",
    "                    self.configer.get('iters'),\n",
    "                    self.scheduler, self.optimizer, backbone_list=[0, ]\n",
    "                )\n",
    "\n",
    "            (inputs, targets), batch_size = self.data_helper.prepare_data(data_dict)\n",
    "            self.data_time.update(time.time() - start_time)\n",
    "\n",
    "            foward_start_time = time.time()\n",
    "            with torch.cuda.amp.autocast():\n",
    "                if not self.with_proto:\n",
    "                    outputs = self.seg_net(*inputs)\n",
    "                else:\n",
    "                    pretrain_prototype = True if self.configer.get('iters') < self. configer.get('protoseg', 'warmup_iters') else False\n",
    "                    outputs = self.seg_net(*inputs, gt_semantic_seg=targets[:, None, ...],\n",
    "                                           pretrain_prototype=pretrain_prototype)\n",
    "            self.foward_time.update(time.time() - foward_start_time)\n",
    "\n",
    "            loss_start_time = time.time()\n",
    "            if is_distributed():\n",
    "                import torch.distributed as dist\n",
    "                def reduce_tensor(inp):\n",
    "                    \"\"\"\n",
    "                    Reduce the loss from all processes so that \n",
    "                    process with rank 0 has the averaged results.\n",
    "                    \"\"\"\n",
    "                    world_size = get_world_size()\n",
    "                    if world_size < 2:\n",
    "                        return inp\n",
    "                    with torch.no_grad():\n",
    "                        reduced_inp = inp\n",
    "                        dist.reduce(reduced_inp, dst=0)\n",
    "                    return reduced_inp\n",
    "\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    loss = self.pixel_loss(outputs, targets)\n",
    "                    backward_loss = loss\n",
    "                    display_loss = reduce_tensor(backward_loss) / get_world_size()\n",
    "            else:\n",
    "                backward_loss = display_loss = self.pixel_loss(outputs, targets)\n",
    "\n",
    "            self.train_losses.update(display_loss.item(), batch_size)\n",
    "            self.loss_time.update(time.time() - loss_start_time)\n",
    "\n",
    "            backward_start_time = time.time()\n",
    "\n",
    "            # backward_loss.backward()\n",
    "            # self.optimizer.step()\n",
    "            scaler.scale(backward_loss).backward()\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            self.backward_time.update(time.time() - backward_start_time)\n",
    "\n",
    "            # Update the vars of the train phase.\n",
    "            self.batch_time.update(time.time() - start_time)\n",
    "            start_time = time.time()\n",
    "            self.configer.plus_one('iters')\n",
    "\n",
    "            # Print the log info & reset the states.\n",
    "            if self.configer.get('iters') % self.configer.get('solver', 'display_iter') == 0 and \\\n",
    "                    (not is_distributed() or get_rank() == 0):\n",
    "                \n",
    "                self.batch_time.reset()\n",
    "                self.foward_time.reset()\n",
    "                self.backward_time.reset()\n",
    "                self.loss_time.reset()\n",
    "                self.data_time.reset()\n",
    "                self.train_losses.reset()\n",
    "\n",
    "            # save checkpoints for swa\n",
    "            if 'swa' in self.configer.get('lr', 'lr_policy') and \\\n",
    "                    self.configer.get('iters') > normal_max_iters and \\\n",
    "                    ((self.configer.get('iters') - normal_max_iters) % swa_step_max_iters == 0 or \\\n",
    "                     self.configer.get('iters') == self.configer.get('solver', 'max_iters')):\n",
    "                self.optimizer.update_swa()\n",
    "\n",
    "            if self.configer.get('iters') == self.configer.get('solver', 'max_iters'):\n",
    "                break\n",
    "\n",
    "            # Check to val the current model.\n",
    "            # if self.configer.get('epoch') % self.configer.get('solver', 'test_interval') == 0:\n",
    "            if self.configer.get('iters') % self.configer.get('solver', 'test_interval') == 0:\n",
    "                self.__val()\n",
    "\n",
    "        self.configer.plus_one('epoch')\n",
    "\n",
    "    def __val(self, data_loader=None):\n",
    "        \"\"\"\n",
    "          Validation function during the train phase.\n",
    "        \"\"\"\n",
    "        self.seg_net.eval()\n",
    "        self.pixel_loss.eval()\n",
    "        start_time = time.time()\n",
    "        replicas = self.evaluator.prepare_validaton()\n",
    "\n",
    "        data_loader = self.val_loader if data_loader is None else data_loader\n",
    "        for j, data_dict in enumerate(data_loader):\n",
    "            if j % 10 == 0:\n",
    "                if is_distributed(): dist.barrier()  # Synchronize all processes\n",
    "\n",
    "            if self.configer.get('dataset') == 'lip':\n",
    "                (inputs, targets, inputs_rev, targets_rev), batch_size = self.data_helper.prepare_data(data_dict,\n",
    "                                                                                                       want_reverse=True)\n",
    "            else:\n",
    "                (inputs, targets), batch_size = self.data_helper.prepare_data(data_dict)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if self.configer.get('dataset') == 'lip':\n",
    "                    inputs = torch.cat([inputs[0], inputs_rev[0]], dim=0)\n",
    "                    outputs = self.seg_net(inputs)\n",
    "                    if not is_distributed():\n",
    "                        outputs_ = self.module_runner.gather(outputs)\n",
    "                    else:\n",
    "                        outputs_ = outputs\n",
    "                    if isinstance(outputs_, (list, tuple)):\n",
    "                        outputs_ = outputs_[-1]\n",
    "                    outputs = outputs_[0:int(outputs_.size(0) / 2), :, :, :].clone()\n",
    "                    outputs_rev = outputs_[int(outputs_.size(0) / 2):int(outputs_.size(0)), :, :, :].clone()\n",
    "                    if outputs_rev.shape[1] == 20:\n",
    "                        outputs_rev[:, 14, :, :] = outputs_[int(outputs_.size(0) / 2):int(outputs_.size(0)), 15, :, :]\n",
    "                        outputs_rev[:, 15, :, :] = outputs_[int(outputs_.size(0) / 2):int(outputs_.size(0)), 14, :, :]\n",
    "                        outputs_rev[:, 16, :, :] = outputs_[int(outputs_.size(0) / 2):int(outputs_.size(0)), 17, :, :]\n",
    "                        outputs_rev[:, 17, :, :] = outputs_[int(outputs_.size(0) / 2):int(outputs_.size(0)), 16, :, :]\n",
    "                        outputs_rev[:, 18, :, :] = outputs_[int(outputs_.size(0) / 2):int(outputs_.size(0)), 19, :, :]\n",
    "                        outputs_rev[:, 19, :, :] = outputs_[int(outputs_.size(0) / 2):int(outputs_.size(0)), 18, :, :]\n",
    "                    outputs_rev = torch.flip(outputs_rev, [3])\n",
    "                    outputs = (outputs + outputs_rev) / 2.\n",
    "                    self.evaluator.update_score(outputs, data_dict['meta'])\n",
    "\n",
    "                elif self.data_helper.conditions.diverse_size:\n",
    "                    if is_distributed():\n",
    "                        outputs = [self.seg_net(inputs[i]) for i in range(len(inputs))]\n",
    "                    else:\n",
    "                        outputs = nn.parallel.parallel_apply(replicas[:len(inputs)], inputs)\n",
    "\n",
    "                    for i in range(len(outputs)):\n",
    "                        loss = self.pixel_loss(outputs[i], targets[i].unsqueeze(0))\n",
    "                        # self.val_losses.update(loss.item(), 1)\n",
    "                        outputs_i = outputs[i]\n",
    "                        if isinstance(outputs_i, torch.Tensor):\n",
    "                            outputs_i = [outputs_i]\n",
    "                        self.evaluator.update_score(outputs_i, data_dict['meta'][i:i + 1])\n",
    "\n",
    "                else:\n",
    "                    outputs = self.seg_net(*inputs)\n",
    "\n",
    "                    if not is_distributed():\n",
    "                        outputs = self.module_runner.gather(outputs)\n",
    "                    if isinstance(outputs, dict):\n",
    "                        outputs = outputs['seg']\n",
    "                    self.evaluator.update_score(outputs, data_dict['meta'])\n",
    "\n",
    "            self.batch_time.update(time.time() - start_time)\n",
    "            start_time = time.time()\n",
    "\n",
    "        self.evaluator.update_performance()\n",
    "\n",
    "        self.module_runner.save_net(self.seg_net, save_mode='performance')\n",
    "        cudnn.benchmark = True\n",
    "\n",
    "        # Print the log info & reset the states.\n",
    "        self.evaluator.reduce_scores()\n",
    "        if not is_distributed() or get_rank() == 0:\n",
    "            self.evaluator.print_scores()\n",
    "\n",
    "        self.batch_time.reset()\n",
    "        self.evaluator.reset()\n",
    "        self.seg_net.train()\n",
    "        self.pixel_loss.train()\n",
    "\n",
    "    def train(self):\n",
    "        # cudnn.benchmark = True\n",
    "        # self.__val()\n",
    "        if self.configer.get('network', 'resume') is not None:\n",
    "            if self.configer.get('network', 'resume_val'):\n",
    "                self.__val(data_loader=self.data_loader.get_valloader(dataset='val'))\n",
    "                return\n",
    "            elif self.configer.get('network', 'resume_train'):\n",
    "                print('train')\n",
    "                self.__val(data_loader=self.data_loader.get_valloader(dataset='train'))\n",
    "                return\n",
    "            # return\n",
    "\n",
    "        # if self.configer.get('network', 'resume') is not None and self.configer.get('network', 'resume_val'):\n",
    "        #     self.__val(data_loader=self.data_loader.get_valloader(dataset='val'))\n",
    "        #     return\n",
    "\n",
    "        while self.configer.get('iters') < self.configer.get('solver', 'max_iters'):\n",
    "            self.__train()\n",
    "\n",
    "        # use swa to average the model\n",
    "        if 'swa' in self.configer.get('lr', 'lr_policy'):\n",
    "            self.optimizer.swap_swa_sgd()\n",
    "            self.optimizer.bn_update(self.train_loader, self.seg_net)\n",
    "\n",
    "        self.__val(data_loader=self.data_loader.get_valloader(dataset='val'))\n",
    "\n",
    "    def summary(self):\n",
    "        from lib.utils.summary import get_model_summary\n",
    "        import torch.nn.functional as F\n",
    "        self.seg_net.eval()\n",
    "\n",
    "        for j, data_dict in enumerate(self.train_loader):\n",
    "            print(get_model_summary(self.seg_net, data_dict['img'][0:1]))\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "czech-border",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 15:41:22,012 INFO    Added key: store_based_barrier_key:1 to store for rank: 0\n"
     ]
    }
   ],
   "source": [
    "from lib.utils.helpers.file_helper import FileHelper\n",
    "from lib.utils.helpers.image_helper import ImageHelper\n",
    "from lib.utils.tools.logger import Logger as Log\n",
    "from lib.metrics.running_score import RunningScore\n",
    "from lib.vis.seg_visualizer import SegVisualizer\n",
    "from lib.vis.palette import get_cityscapes_colors\n",
    "from segmentor.tools.module_runner import ModuleRunner\n",
    "from segmentor.tools.optim_scheduler import OptimScheduler\n",
    "\n",
    "import fnmatch\n",
    "import platform\n",
    "\n",
    "import torch.distributed as dist\n",
    "dist.init_process_group('gloo', init_method='file:///tmp/somefile', rank=0, world_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "threaded-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rolled-wrist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass parser set\n"
     ]
    }
   ],
   "source": [
    "configer = Configer(args_parser = main())\n",
    "data_dir = configer.get('data', 'data_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "standing-vulnerability",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 15:41:22,024 INFO    [module_runner.py, 46] BN Type is torchsyncbn.\n",
      "2022-11-06 15:41:22,025 INFO    [__init__.py, 17] Using evaluator: StandardEvaluator\n",
      "2022-11-06 15:41:22,025 INFO    [running_score.py, 129] 19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hrnet_w48_proto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-06 15:41:22,376 INFO    [module_helper.py, 141] Loading pretrained model:/tmp/working/workspace/ProtoSeg_local/checkpoints/cityscapes/hrnetv2_w48_imagenet_pretrained.pth\n",
      "2022-11-06 15:41:22,482 INFO    [module_helper.py, 205] Missing keys: []\n",
      "2022-11-06 15:41:23,899 INFO    [optim_scheduler.py, 96] Use lambda_poly policy with default power 0.9\n",
      "2022-11-06 15:41:23,899 INFO    [<ipython-input-7-fb31557d580a>, 114] use the DefaultLoader for train...\n",
      "2022-11-06 15:41:23,950 INFO    [default_loader.py, 38] train 2975\n",
      "2022-11-06 15:41:23,950 INFO    [<ipython-input-7-fb31557d580a>, 146] use DefaultLoader for val ...\n",
      "2022-11-06 15:41:23,954 INFO    [default_loader.py, 38] val 500\n",
      "2022-11-06 15:41:23,955 INFO    [<ipython-input-9-70c2063975e7>, 21] use loss: pixel_prototype_ce_loss.\n",
      "2022-11-06 15:41:23,955 INFO    [loss_proto.py, 60] ignore_index: -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use distributed loss\n"
     ]
    }
   ],
   "source": [
    "if isinstance(data_dir, str):\n",
    "    data_dir = [data_dir]\n",
    "abs_data_dir = [os.path.expanduser(x) for x in data_dir]\n",
    "configer.update(['data', 'data_dir'], abs_data_dir)\n",
    "            \n",
    "#             project_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "#             configer.add(['project_dir'], project_dir)\n",
    "model = None\n",
    "\n",
    "model =Trainer(configer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "tropical-england",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "trying to initialize the default process group twice!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e04af5847c87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_process_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gloo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'file:///tmp/somefile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworld_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\u001b[0m in \u001b[0;36minit_process_group\u001b[0;34m(backend, init_method, timeout, world_size, rank, store, group_name)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mGroupMember\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWORLD\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m         raise RuntimeError(\"trying to initialize the default process group \"\n\u001b[0m\u001b[1;32m    466\u001b[0m                            \"twice!\")\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: trying to initialize the default process group twice!"
     ]
    }
   ],
   "source": [
    "from lib.utils.helpers.file_helper import FileHelper\n",
    "from lib.utils.helpers.image_helper import ImageHelper\n",
    "from lib.utils.tools.logger import Logger as Log\n",
    "from lib.metrics.running_score import RunningScore\n",
    "from lib.vis.seg_visualizer import SegVisualizer\n",
    "from lib.vis.palette import get_cityscapes_colors\n",
    "from segmentor.tools.module_runner import ModuleRunner\n",
    "from segmentor.tools.optim_scheduler import OptimScheduler\n",
    "\n",
    "import fnmatch\n",
    "import platform\n",
    "\n",
    "import torch.distributed as dist\n",
    "dist.init_process_group('gloo', init_method='file:///tmp/somefile', rank=0, world_size=1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    INFERENCE = True\n",
    "    parser = argparse.ArgumentParser()\n",
    "    if CFG.model_method == 'fcn':\n",
    "        if CFG.phase == 'infer':\n",
    "            print('Load Model Now ...')\n",
    "            configer = Configer(args_parser = main())\n",
    "            data_dir = configer.get('data', 'data_dir')\n",
    "            \n",
    "            if isinstance(data_dir, str):\n",
    "                data_dir = [data_dir]\n",
    "            abs_data_dir = [os.path.expanduser(x) for x in data_dir]\n",
    "            configer.update(['data', 'data_dir'], abs_data_dir)\n",
    "            \n",
    "#             project_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "#             configer.add(['project_dir'], project_dir)\n",
    "            model = None\n",
    "            \n",
    "            model =Tester(configer)\n",
    "            print('Load Model Finish')\n",
    "            \n",
    "            if INFERENCE:\n",
    "                print('Inference Starting ...')\n",
    "                args = inference_fn()\n",
    "                cityscapes_evaluator = CityscapesEvaluator()\n",
    "                cityscapes_evaluator.evaluate(pred_dir=args.pred_dir, gt_dir=args.gt_dir)\n",
    "  \n",
    "        else:\n",
    "            print('Load Model Now ...')\n",
    "            configer = Configer(args_parser = main())\n",
    "            data_dir = configer.get('data', 'data_dir')\n",
    "            \n",
    "            if isinstance(data_dir, str):\n",
    "                data_dir = [data_dir]\n",
    "            abs_data_dir = [os.path.expanduser(x) for x in data_dir]\n",
    "            configer.update(['data', 'data_dir'], abs_data_dir)\n",
    "            \n",
    "#             project_dir = os.path.dirname(os.path.realpath(__file__))\n",
    "#             configer.add(['project_dir'], project_dir)\n",
    "            model = None\n",
    "            \n",
    "            model =Trainer(configer)\n",
    "            print('create model pass')\n",
    "            \n",
    "            print('start training model')\n",
    "            model.train()\n",
    "    else:\n",
    "        print('attention models will be available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-providence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-transformation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unsigned-justice",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-diesel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sublime-savannah",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-barrel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-message",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silent-advertiser",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-guitar",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
